{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any fur\n",
      "------------------------\n",
      "set([u'\\n', u' ', u':', u'C', u'B', u'F', u'a', u'c', u'e', u'd', u'f', u'i', u'o', u'n', u'p', u's', u'r', u'u', u't', u'w', u'y', u'z'])\n",
      "22\n",
      "------------------------\n",
      "{u'\\n': 1, u' ': 5, u':': 1, u'C': 1, u'B': 1, u'F': 1, u'a': 1, u'c': 1, u'e': 6, u'd': 1, u'f': 2, u'i': 3, u'o': 2, u'n': 2, u'p': 1, u's': 1, u'r': 4, u'u': 1, u't': 2, u'w': 1, u'y': 1, u'z': 1}\n",
      "------------------------\n",
      "[(u'\\n', 1), (u' ', 5), (u':', 1), (u'C', 1), (u'B', 1), (u'F', 1), (u'a', 1), (u'c', 1), (u'e', 6), (u'd', 1), (u'f', 2), (u'i', 3), (u'o', 2), (u'n', 2), (u'p', 1), (u's', 1), (u'r', 4), (u'u', 1), (u't', 2), (u'w', 1), (u'y', 1), (u'z', 1)]\n",
      "------------------------\n",
      "[(u'e', 6), (u' ', 5), (u'r', 4), (u'i', 3), (u'f', 2), (u'o', 2), (u'n', 2), (u't', 2), (u'\\n', 1), (u':', 1), (u'C', 1), (u'B', 1), (u'F', 1), (u'a', 1), (u'c', 1), (u'd', 1), (u'p', 1), (u's', 1), (u'u', 1), (u'w', 1), (u'y', 1), (u'z', 1)]\n",
      "------------------------\n",
      "[(u'e', 6), (u' ', 5), (u'r', 4), (u'i', 3), (u'f', 2), (u'o', 2), (u'n', 2), (u't', 2), (u'\\n', 1), (u':', 1), (u'C', 1), (u'B', 1), (u'F', 1), (u'a', 1), (u'c', 1), (u'd', 1), (u'p', 1), (u's', 1), (u'u', 1), (u'w', 1), (u'y', 1), (u'z', 1)]\n",
      "------------------------\n",
      "[u'e', u' ', u'r', u'i', u'f', u'o', u'n', u't', u'\\n', u':', u'C', u'B', u'F', u'a', u'c', u'd', u'p', u's', u'u', u'w', u'y', u'z']\n",
      "------------------------\n",
      "{u'\\n': 8, u' ': 1, u':': 9, u'C': 10, u'B': 11, u'F': 12, u'a': 13, u'c': 14, u'e': 0, u'd': 15, u'f': 4, u'i': 3, u'o': 5, u'n': 6, u'p': 16, u's': 17, u'r': 2, u'u': 18, u't': 7, u'w': 19, u'y': 20, u'z': 21}\n",
      "------------------------\n",
      "{0: u'e', 1: u' ', 2: u'r', 3: u'i', 4: u'f', 5: u'o', 6: u'n', 7: u't', 8: u'\\n', 9: u':', 10: u'C', 11: u'B', 12: u'F', 13: u'a', 14: u'c', 15: u'd', 16: u'p', 17: u's', 18: u'u', 19: u'w', 20: u'y', 21: u'z'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import codecs\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "# from utils import TextConverter, batch_generator\n",
    "\n",
    "\n",
    "def batch_generator(arr, n_seqs, n_steps):\n",
    "    arr = copy.copy(arr)\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(arr) / batch_size)\n",
    "    arr = arr[:batch_size * n_batches]\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    while True:\n",
    "        np.random.shuffle(arr)\n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            x = arr[:, n:n + n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "class TextConverter(object):\n",
    "    def __init__(self, text=None, max_vocab=5000, filename=None):\n",
    "        if filename is not None:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.vocab = pickle.load(f)\n",
    "        else:\n",
    "            vocab = set(text)\n",
    "            print('------------------------')\n",
    "            print(vocab)\n",
    "            print(len(vocab))\n",
    "            # max_vocab_process\n",
    "            vocab_count = {}\n",
    "            for word in vocab:\n",
    "                vocab_count[word] = 0\n",
    "            for word in text:\n",
    "                vocab_count[word] += 1\n",
    "            print('------------------------')\n",
    "            print(vocab_count)\n",
    "            vocab_count_list = []\n",
    "            for word in vocab_count:\n",
    "                vocab_count_list.append((word, vocab_count[word]))\n",
    "            print('------------------------')\n",
    "            print(vocab_count_list)\n",
    "            vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            print('------------------------')\n",
    "            print(vocab_count_list)\n",
    "            if len(vocab_count_list) > max_vocab:\n",
    "                vocab_count_list = vocab_count_list[:max_vocab]\n",
    "            print('------------------------')\n",
    "            print(vocab_count_list)\n",
    "            vocab = [x[0] for x in vocab_count_list]\n",
    "            print('------------------------')\n",
    "            print(vocab)\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
    "        print('------------------------')\n",
    "        print(self.word_to_int_table)\n",
    "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
    "        print('------------------------')\n",
    "        print(self.int_to_word_table)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        if word in self.word_to_int_table:\n",
    "            return self.word_to_int_table[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def int_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.int_to_word_table[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_arr(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_int(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def arr_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.int_to_word(index))\n",
    "        return \"\".join(words)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.vocab, f)\n",
    "\n",
    "with codecs.open('Char-RNN-TensorFlow/data/shakespeare.txt', encoding='utf-8') as f:\n",
    "    text = f.read(40)\n",
    "print(text)\n",
    "converter = TextConverter(text, 40)\n",
    "converter.save_to_file(os.path.join('./', 'converter.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "23\n",
      "First Citizen:\n",
      "Before we proceed any fur\n",
      "[12  3  2 17  7  1 10  3  7  3 21  0  6  9  8 11  0  4  5  2  0  1 19  0\n",
      "  1 16  2  5 14  0  0 15  1 13  6 20  1  4 18  2]\n"
     ]
    }
   ],
   "source": [
    "print('------------------------')\n",
    "print(converter.vocab_size)\n",
    "print(text)\n",
    "arr = converter.text_to_arr(text)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  3  2 17  7  1 10  3  7  3 21  0  6  9  8 11  0  4  5  2]\n",
      " [ 0  1 19  0  1 16  2  5 14  0  0 15  1 13  6 20  1  4 18  2]]\n",
      "-----------------------\n",
      "[[ 3  2 17  7  1 10  3  7  3 21  0  6  9  8 11  0  4  5  2 12]\n",
      " [ 1 19  0  1 16  2  5 14  0  0 15  1 13  6 20  1  4 18  2  0]]\n",
      "=======================\n",
      "[[12  3  2 17  7  1 10  3  7  3 21  0  6  9  8 11  0  4  5  2]\n",
      " [ 0  1 19  0  1 16  2  5 14  0  0 15  1 13  6 20  1  4 18  2]]\n",
      "-----------------------\n",
      "[[ 3  2 17  7  1 10  3  7  3 21  0  6  9  8 11  0  4  5  2 12]\n",
      " [ 1 19  0  1 16  2  5 14  0  0 15  1 13  6 20  1  4 18  2  0]]\n",
      "=======================\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "g = batch_generator(arr, 2, 20)\n",
    "count = 0\n",
    "for x, y in g:\n",
    "    a = x\n",
    "    b = y\n",
    "    print(a)\n",
    "    print('-----------------------')\n",
    "    print(b)\n",
    "    print('=======================')\n",
    "    count += 1\n",
    "    if count >= 2:\n",
    "        break\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    # 将除了top_n个预测值的位置都置为0\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    # 归一化概率\n",
    "    p = p / np.sum(p)\n",
    "    # 随机选取一个字符\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c\n",
    "predictions = [0.01, 0.02, 0.3, 0.4, 0.8, 0.06, 0.01, 0.02, 0.01, 0.03]\n",
    "vocab_size = 10\n",
    "print(pick_top_n(predictions, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189448\n",
      "200\n",
      "time: 78.2641119957\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# first, make sure the max length of a session\n",
    "# 189448\n",
    "# 200\n",
    "# time: 78.2641119957\n",
    "with open('data/clicks.dat', 'r') as rf:\n",
    "    max_length = 0\n",
    "    cache_length = 0\n",
    "    cache_id = 0\n",
    "    max_id = 0\n",
    "    start = time.time()\n",
    "    for line in rf.readlines():\n",
    "#         print(line)\n",
    "        current_id = int(line.split(',')[0])\n",
    "        if current_id != cache_id:            \n",
    "            if max_length < cache_length:\n",
    "                max_length = cache_length\n",
    "                max_id = cache_id\n",
    "            cache_id = current_id\n",
    "            cache_length = 0\n",
    "        cache_length += 1\n",
    "    end = time.time()\n",
    "    print(max_id)\n",
    "    print(max_length)\n",
    "    print('time: {}'.format((end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52739\n",
      "214687796\n",
      "214691443\n",
      "214575686\n",
      "correct!\n"
     ]
    }
   ],
   "source": [
    "# next, statisitcs the num of different item_id and make a map\n",
    "# with open('data/clicks.dat', 'r') as rf:\n",
    "# item_num 52739\n",
    "with open('data/clicks.dat', 'r') as rf:\n",
    "    item_set = set()\n",
    "    for line in rf.readlines():\n",
    "#         print(line)\n",
    "        item_set.add(line.split(',')[2])\n",
    "#     print(item_set)\n",
    "    print(len(item_set))\n",
    "item_new_id = 1\n",
    "map_dict = {}\n",
    "for i in item_set:\n",
    "    map_dict[str(item_new_id)] = i\n",
    "    item_new_id += 1\n",
    "print(map_dict['1'])\n",
    "print(map_dict['100'])\n",
    "print(map_dict['52739'])\n",
    "if '52740' not in map_dict:\n",
    "    print('correct!')\n",
    "else:\n",
    "    print(map_dict['52740'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281626,2014-04-06T09:20:42.331Z,214535653,0\n",
      "\n",
      "281626,2014-04-06T09:22:05.402Z,214819357,0\n",
      "\n",
      "281626,2014-04-06T09:22:58.720Z,214535653,0\n",
      "\n",
      "281626,2014-04-06T09:23:15.534Z,214535653,0\n",
      "\n",
      "281626,2014-04-06T09:24:16.421Z,214821277,0\n",
      "\n",
      "281626,2014-04-06T09:26:07.764Z,214684513,0\n",
      "\n",
      "281626,2014-04-06T09:28:15.255Z,214535681,0\n",
      "\n",
      "281626,2014-04-06T09:29:00.938Z,214552370,0\n",
      "\n",
      "281626,2014-04-06T09:31:52.830Z,214698577,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/clicks.dat', 'r') as rf:\n",
    "    for line in rf.readlines():\n",
    "        if line.split(',')[0] == '281626':\n",
    "            print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 not in\n",
      "1\n",
      "2\n",
      "{'2': 'b'}\n"
     ]
    }
   ],
   "source": [
    "test_dict = {'1': 'a', '2': 'b'}\n",
    "if '1' not in test_dict:\n",
    "    print('1 not in')\n",
    "if '3' not in test_dict.keys():\n",
    "    print('3 not in')\n",
    "for i in test_dict:\n",
    "    print(i)\n",
    "test_dict.pop('1')\n",
    "print(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [0, 6, 7, 8, 9]\n",
    "result = list(set(a).intersection(set(b)))\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([1, 2, 3, 4])\n",
      "[1, 2, 3, 4]\n",
      "<type 'list'>\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "test = [1, 2, 3, 4, 4, 3, 2, 1]\n",
    "result = set()\n",
    "for i in test:\n",
    "    result.add(i)\n",
    "print(result)\n",
    "test = list(result)\n",
    "print(test)\n",
    "print(type(test))\n",
    "result.clear()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
